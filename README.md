# ğŸ“Š Lakehouse360: End-to-End Data Engineering & Interactive Analytics Platform

**Lakehouse360** is a modular, open-source data engineering and analytics project built to demonstrate a complete pipeline from raw data ingestion to insightful interactive dashboards using only open-source tools like Python, DuckDB, and Streamlit.

---

## ğŸ“ Project Structure

lakehouse360/
â”‚
â”œâ”€â”€ data/                     # Raw and intermediate data files
â”‚   â”œâ”€â”€ json_files/           # Converted JSON records
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ cleaned_parquet/      # Cleaned Parquet outputs
â”‚
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ convert_to_json.py    # Converts CSV, JSONL, Parquet to unified JSON
â”‚   â”œâ”€â”€ clean_and_export.py   # Cleans and exports to Parquet using DuckDB
â”‚
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ validate_data.py      # Validates data with Pydantic models
â”‚   â”œâ”€â”€ profile_data.py       # Profiles JSON data using DuckDB
â”‚
â”œâ”€â”€ fixes/
â”‚   â”œâ”€â”€ patch_orders_product_ids.py  # Fix missing product_id references in orders
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ duckdb_analytics.py   # Runs analytics using DuckDB and Pandas
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ dashboard.py          # Interactive dashboard using Streamlit
â”‚   â”œâ”€â”€ report_utils.py       # PDF report generation using fpdf
â”‚
â””â”€â”€ README.md                 # This file

---

## âœ… Features

- **Automatic data ingestion** from CSV, TSV, Parquet, and JSONL formats
- **Validation** with Pydantic to catch schema mismatches
- **Profiling** with DuckDB for column-level summaries
- **Cleaning & Transformation** using SQL within DuckDB
- **Advanced analytics** using Pandas and SQL queries
- **Streamlit Dashboard** for:
  - Complex filtering (multi-column, multi-select)
  - Visualizations (bar, line, pie charts via Plotly)
  - Summary metrics (row count, null count, etc.)
  - Export options:
    - Filtered data (CSV, Excel, JSON)
    - PDF reports with metadata and top records

---

## ğŸš€ Getting Started

### 1. Clone the repo

git clone https://github.com/ojasshukla01/lakehouse360.git
cd lakehouse360

### 2. Set up virtual environment

python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt

### 3. Run the Pipeline

# Convert data
python transform/convert_to_json.py

# Validate and clean
python validation/validate_data.py
python transform/clean_and_export.py

# Analyze
python analysis/duckdb_analytics.py

# Launch dashboard
streamlit run streamlit_app/dashboard.py

---

## ğŸ“¦ Dependencies

- pandas
- duckdb
- streamlit
- fpdf
- openpyxl
- pyarrow
- plotly

Install them via:

pip install -r requirements.txt

---

## ğŸ“„ Author

Built by **Ojas Shukla**  
ğŸ“ Data Engineer | Python | GCP | Streamlit | DuckDB | OSS Contributor

---

## ğŸ’¡ Future Plans

- Scheduled automation with `cron` or `GitHub Actions`
- Dockerized deployment
- Integration with cloud storage (e.g., GCS/S3)
- Alerting and data quality monitoring

---

## ğŸ“¬ Feedback

Found an issue or want to contribute? Feel free to open a pull request or create an issue!